{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensors are generalisations of vectors and matrices to higher dimensions. They're vastly used on neural networks representations of data! \n",
    "\n",
    "* a 0D tensor (tensor of rank 0) is a scalar (a number)\n",
    "* a 1D tensor (tensor of rank 1) is a vector\n",
    "* a 2D tensor (...) is a matrix\n",
    "* a 3D tensor is ... a 3D tensor\n",
    "* ...\n",
    "\n",
    "Each dimension of a tensor is called an *axis*. Note that you can create arrays of any rank with Numpy! \n",
    "\n",
    "## Operations\n",
    "\n",
    "Operations on tensors are the scaled up versions of operations on vectors and matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors for data representation in Deep Learning\n",
    "\n",
    "When passing a training set to a deep model, it is customary to place each sample in an array and build a tensor.\n",
    "\n",
    "Suppose you're working on some images task for instance. Images are matrices (when grayscale), that is, tensors of rank 2. The typical way to pass a training set of images to a deep network is via the construction of a tensor of rank 3 where all images are placed in an array, effectively where the first axis denotes the sample. Same with colour images, which are tensors of rank 3, the third dimension being the colour channel; a set of those is a tensor of rank 4 where the first axis is the sample, the second and third are height and width and the fourth the colour.\n",
    "\n",
    "Note that Tensorflow uses the convention here described, Theano puts the colour channel on the second axis, and then height and width instead. \n",
    "\n",
    "This structure is employed for each sort of dimensionality: you build a tensor where the first axes stores each sample. Videos go on 5D tensors as each frame is an image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Broadcasting is the procedure that makes it possible to compute operations over tensors of different rank, like for instance an addition between a vector and a matrix. What broadcasting does is \"extending\" the smallest of the tensors by replicating it across the missing axes so that it matches the shape of the other tensor. For example, if you want to sum vector $v = (1, 2, 2)$ to matrix $A = \\begin{bmatrix}\n",
    "    2  & 3 & 1 \\\\\n",
    "    1  & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$, you replicate the vector over the missing axis to build a matrix, effectively then summing $V = \\begin{bmatrix}\n",
    "    1  & 2 & 2 \\\\\n",
    "    1  & 2 & 2\n",
    "\\end{bmatrix}$ to $A$, which yields $\\begin{bmatrix}\n",
    "    3  & 5 & 3 \\\\\n",
    "    2  & 3 & 3\n",
    "\\end{bmatrix}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic computation\n",
    "\n",
    "\"This means that, given a chain of operations with a known derivative, they can compute a gradient function for the chain (by applying the chain rule) that maps network parameter values to gradient values. When you have access to such a function, the backward pass is reduced to a call to this gradient function. Thanks to symbolic differentiation, you’ll never have to implement the Backpropagation algorithm by hand.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. F Chollet, **Deep Learning with Python**, *Manning*, 2017\n",
    "2. [**TensorFlow** on tensors](https://www.tensorflow.org/programmers_guide/tensors)\n",
    "3. [**TensorFlow on broadcasting**](https://www.tensorflow.org/performance/xla/broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
