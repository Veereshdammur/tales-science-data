{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "    /* DOWNLOAD COMPUTER MODERN FONT JUST IN CASE */\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "\n",
       "    /* GLOBAL TEXT FONT */\n",
       "    div#notebook,\n",
       "    div.output_area pre,\n",
       "    div.output_wrapper,\n",
       "    div.prompt {\n",
       "      font-family: Times new Roman, monospace !important;\n",
       "    }\n",
       "\n",
       "    /* CENTER FIGURE */\n",
       "    .output_png {\n",
       "        display: table-cell;\n",
       "        text-align: center;\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    /* LINK */\n",
       "    a {\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H1 */\n",
       "    h1 {\n",
       "        font-size: 42px !important;\n",
       "        text-align: center;\n",
       "        color: #FF8000;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h2 {\n",
       "        font-size: 32px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h3 {\n",
       "        font-size: 24px !important;\n",
       "    }\n",
       "\n",
       "    /* H2 */\n",
       "    h4 {\n",
       "        font-size: 20px !important;\n",
       "    }\n",
       "\n",
       "    /* PARAGRAPH */\n",
       "    p {\n",
       "        font-size: 16px !important;\n",
       "        text-align: center;\n",
       "    }\n",
       "\n",
       "    /* LIST ITEM */\n",
       "    li {\n",
       "        font-size: 16px !important;\n",
       "    }\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run ../../common/import_all.py\n",
    "\n",
    "from common.setup_notebook import set_css_style, setup_matplotlib, config_ipython\n",
    "\n",
    "config_ipython()\n",
    "setup_matplotlib()\n",
    "set_css_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a model\n",
    "\n",
    "The performance of the model you've built should always be evaluated on a separate set of data which has not been seen (in any way) during the training phase. This ensures that the evaluation is unbiased and clean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts: how to in a nutshell\n",
    "\n",
    "### Training and test set\n",
    "\n",
    "When you have one model candidate only and at the end of your modelling efforts you're tasked with evaluating how good it is, you need to have the separate dataset, usually called the *test set* to run it on. The usual way to do this is to, at the start of any modelling work, take aside a portion of the whole dataset you're using to constitute this test set, and train the model on the rest, which gets called the *training set*. Usual splits are 70% training and 30% test, or 80/20, depends on how much data you have in the first place really, as you don't want to remove too much from the training set.\n",
    "\n",
    "Note that this is really essential! Evaluating performance metrics on the training set will typically give you higher figures: the model has been trained there, so will have learned the patterns on these data. It's when you use new data that you see the actual performance and how good your model is in generalising.\n",
    "\n",
    "Also note that in the spirit of not using the test set in the modelling phase at all to avoid polluting the subsequent evaluation, in the cases when you have rescaled the data in training and need to now do on the test set (because you need rescaling there as well), you need to use the metrics of the training set, not those of the test set! So say if you have subtracted the mean and divided by the standard deviation in the training set, the right thing to do is use mean and std of the training set on the test set as well. Otherwise you'd be using something that is \n",
    "\n",
    "### Training, test and validation sets\n",
    "\n",
    "When you are evaluating which of several models performs best in your case, the way to go is to divide the original dataset into three parts:\n",
    "\n",
    "* a *training* set to fit the models\n",
    "* a *test* set to evaluate the final model\n",
    "* a *validation* set to select the model\n",
    "\n",
    "While in those cases when you are evaluating one model only you are fine with a split into training/test, when you have multiple rivalling models to choose from you need a split into three. A typical way is to split into 50%/25%/25% but it really varies depending on the size you start with in the first place. Rivalling models can actually be different algorithms employed, or the same algorithm but with different parameters, so this procedure is also used when you need to fine-tune.\n",
    "\n",
    "The test set is to be kept aside and only used at the very end of the analysis, to estimate the generalisation error at the end. The validation set is used to assess the performance of the specific model you are evaluating. The model you will run on the test set at the end will be the one with the best performance in the validation phase. \n",
    "\n",
    "If for these types of cases you'd use only a 2-way split you'd risk underestimating the error and overfitting, because you'd have chosen the model by tuning it on a set and it may not generalise well to unseen data. \n",
    "\n",
    "The training set is used to train each model (or each combination of parameters) on the data; the validation phase then assess, for each of these models/tunes, the performance. After it, you'd have selected the one giving the best result. This phase can for instance encompass a cross-validation. \n",
    "The testing phase is then needed to have an unbiased estimation of the generalisation error, as the model will run on fresh, independent data. \n",
    "\n",
    "What to typically do when you have finished the whole procedure in order to have a usable model, is to re-train it on training+validation sets and then test it on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques: Cross Validation\n",
    "\n",
    "### What do you do in cross-validation?\n",
    "\n",
    "Cross validation is a technique for validation the performance of a model. In its basic form it basically consists in dividing the original data sample into sets, picking one of them as the training set and validating the performance on the other, the test set, repeating the procedure multiple times with different splits of the original set. Eventually, the results are averaged. \n",
    "\n",
    "The procedure results in a better outcome than the simple training/test split because it allows for a control of the error by the averaging procedure.\n",
    "\n",
    "There are multiple categories of cross-validation techniques. \n",
    "\n",
    "### Types of Cross Validation techniques\n",
    "\n",
    "#### $k$-fold cross validation\n",
    "\n",
    "In this case, the original data set is split into $k$ equally-sized sub-samples and each subset is in turn used as the test set while the remaining $k-1$ constitute the training set. This way you repeat the procedure $k$ times (called *folds*), one for each test set. At the end, an average of all validation results is computed. This way, all sample points in the original set are used both for training and for testing (in different folds). \n",
    "\n",
    "If folds are selected such that each set contains the same percentage of samples in each target class (or dependent variable in the case of regression), it is called a *stratified k-fold cross validation*.\n",
    "\n",
    "#### Leave-one-out\n",
    "\n",
    "Is the variation of the $k$-fold when $k=n$, $n$ being the number of samples, meaning you are doing samples of one data point so it's one against everyone else. \n",
    "\n",
    "#### Leave-$p$-out\n",
    "\n",
    "It is the same as the leave-one-out but the test set is constituted by $p$ of the samples, but all possible splits are calculated, meaning that all possible situations where $p$ items are selected as the test set are built. It is a very expensive procedure and the comprehensive way to consider all possible splits. Note that a $k$-fold is an approximation of this one as not all splits are considered (because the original set is preliminarly partitioned into $k$ subsets).\n",
    "\n",
    "#### Hold-out\n",
    "\n",
    "It is a $k$-fold where $k=2$ but points are randomly assigned to each of the two sets, with typically the training set being bigger. It is a very loose way to do a cross validation, the only real advantage beyond speed being the fact that both sets are large. \n",
    "\n",
    "\n",
    "#### Repeated random sub-sampling validation\n",
    "\n",
    "Also called a Monte Carlo cross validation, this method splits the original data set into training and test randomly at each iteration. The advantage of this method over a $k$ fold is that the number of points into training and test parts does not depend on the number of folds chosen; the disadvantage of this method is that some samples may never be selected in the test set, or selected multiple times over the iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
